{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db651b79-350d-45ad-9f41-6e3c7777f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import torch, logging\n",
    "\n",
    "## disable warnings\n",
    "logging.disable(logging.WARNING)  \n",
    "\n",
    "## Imaging  library\n",
    "from PIL import Image\n",
    "from torchvision import transforms as tfms\n",
    "\n",
    "## Basic libraries\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## For video display\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "## Import the CLIP artifacts \n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd5c4e2-f334-411e-af21-88e8edced315",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n",
    "\n",
    "# Set the device to MPS (Metal Performance Shaders) for Apple Silicon Macs\n",
    "# This allows for GPU acceleration on compatible devices\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675feab-9493-44e8-9471-1666c69262a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(p):\n",
    "    '''\n",
    "    Function to load images from a defined path to Python Imaging Library (PIL) format\n",
    "    '''\n",
    "    # .resize((512,512)): This is the standard input size for Stable Diffusion v1.4\n",
    "    return Image.open(p).convert('RGB').resize((512,512))\n",
    "\n",
    "def convert_pil_to_latents(image):\n",
    "    '''\n",
    "    Function to convert PIL image to latents\n",
    "    '''\n",
    "    # 1. Convert PIL image to tensor\n",
    "    # 2. .unsqueeze(0)\n",
    "    #   * Adds a batch dimension to the tensor\n",
    "    #   * Required for the VAE model which expects a batch of images\n",
    "    # 3. Multiplying by 2.0  and subtracting 1.0: \n",
    "    #   * Normalizes pixel values from range [0, 1] to range [-1, 1]\n",
    "    #   * This is the standard normalization for Stable Diffusion models\n",
    "    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n",
    "    init_image = init_image.to(device=\"mps\", dtype=torch.float16) \n",
    "   \n",
    "    # 1. Encode the image to get the latent distribution\n",
    "    # 2. Sample from the latent distribution (introduces stochasticity; alternative is .mode() for deterministic)\n",
    "    # 3. Scale by 0.18215 to match the model's latent space (standard deviation of the latent space for Stable Diffusion v1.4)\n",
    "    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n",
    "    return init_latent_dist.to(\"mps\")\n",
    "\n",
    "def convert_latents_to_pil(latents):\n",
    "    '''\n",
    "    Convert latent space representations back to viewable PIL images.\n",
    "\n",
    "    Args:\n",
    "        latents (torch.Tensor): Latent space tensor with shape [batch_size, 4, height//8, width//8].\n",
    "        Expected format: float16 tensor on MPS device, scaled by 0.18215.\n",
    "        Example shapes: [1, 4, 64, 64] for 512x512 output images.\n",
    "    \n",
    "    Returns:\n",
    "        list[PIL.Image]: List of PIL Image objects in RGB format.\n",
    "    '''\n",
    "\n",
    "    # 1. Scale the latents by 1 / 0.18215 to match the model's latent space\n",
    "    latents = (1 / 0.18215) * latents\n",
    "    # 2. Decode the latents using the VAE model to get the image tensor\n",
    "    with torch.no_grad(): # Disables gradient computation (saves memory, faster inference)\n",
    "        image = vae.decode(latents).sample\n",
    "    # 3. Normalize the image tensor to range [0, 1]\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    # 4: Convert to numpy and rearrange dimensions\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    # 5. Convert the images to uint8 format for display\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    # 6: Create PIL images\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images\n",
    "\n",
    "def text_encode(prompts, maxlen=None):\n",
    "    \"\"\"\n",
    "    Convert text prompts into CLIP embeddings for diffusion model conditioning.\n",
    "    \n",
    "    Args:\n",
    "        prompts (str or list[str]): Text prompt(s) to encode into embeddings.\n",
    "        maxlen (int, optional): Maximum sequence length for tokenization.\n",
    "            If None, uses tokenizer's default max length (typically 77 tokens).\n",
    "            Longer prompts will be truncated, shorter ones padded.\n",
    "            Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: CLIP text embeddings tensor with shape [batch_size, 77, 768].\n",
    "            - batch_size: Number of prompts\n",
    "            - 77: Maximum token sequence length (CLIP standard)\n",
    "            - 768: Embedding dimension (feature vector size)\n",
    "            Tensor is in float16 format on MPS device for memory efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    if maxlen is None: maxlen = tokenizer.model_max_length\n",
    "    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n",
    "    return text_encoder(inp.input_ids.to(\"mps\"))[0].half()\n",
    "\n",
    "def clear_memory():\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def generate_image(prompts, \n",
    "                   g=7.5, \n",
    "                   seed=100, \n",
    "                   steps=70, \n",
    "                   dim=512, \n",
    "                   save_int=True, \n",
    "                   input_image=None, \n",
    "                   strength=0.5, \n",
    "                   scheduler=None, \n",
    "                   unet=None):\n",
    "    \"\"\"\n",
    "    Generate images using Stable Diffusion with optional image-to-image conditioning.\n",
    "    \n",
    "    This function implements the complete diffusion pipeline: text encoding, image conditioning,\n",
    "    iterative denoising, and final image conversion. It supports both text-to-image and \n",
    "    image-to-image generation with classifier-free guidance.\n",
    "    \n",
    "    Args:\n",
    "            prompts (list[str]): List of text prompts describing the desired image(s).\n",
    "            Example: [\"A banana running with a cocktail glass\"]\n",
    "            \n",
    "            g (float, optional): Guidance scale for classifier-free guidance. Higher values \n",
    "            follow the prompt more closely but may reduce creativity. Defaults to 7.5.\n",
    "            Range: 1.0 (no guidance) to 20.0 (strong guidance)\n",
    "            \n",
    "            seed (int, optional): Random seed for reproducible generation. Same seed with \n",
    "            same parameters produces identical results. Defaults to 100.\n",
    "            \n",
    "            steps (int, optional): Number of denoising steps. More steps = better quality \n",
    "            but longer generation time. Defaults to 70.\n",
    "            Range: 10-100 (typical: 20-50 for speed, 50-100 for quality)\n",
    "            \n",
    "            dim (int, optional): Output image dimension (square). Must be divisible by 8.\n",
    "            Defaults to 512. Common values: 256, 384, 512, 768.\n",
    "            \n",
    "            save_int (bool, optional): Whether to save and display intermediate images \n",
    "            during generation (every 10 steps). Defaults to True.\n",
    "            \n",
    "            input_image (str, optional): Path to input image for image-to-image generation.\n",
    "            If None, performs text-to-image generation. Defaults to None.\n",
    "            Example: 'img/banana_logo.jpg'\n",
    "            \n",
    "            strength (float, optional): Denoising strength for img2img.\n",
    "                0.0 = no change to input image\n",
    "                1.0 = completely ignore input image (pure text2img)\n",
    "                0.3-0.7 = typical range for style transfer\n",
    "                Defaults to 0.5.\n",
    "    \n",
    "    Returns:\n",
    "            list[PIL.Image]: List of generated PIL Images ready for display or saving.        \n",
    "    \"\"\"\n",
    "\n",
    "    # Defining batch size\n",
    "    bs = len(prompts) \n",
    "\n",
    "    # Converting textual prompts to embedding\n",
    "    text = text_encode(prompts) \n",
    "\n",
    "    # convert image to embedding\n",
    "    if input_image:\n",
    "            image = load_image(input_image)\n",
    "            image_emb = convert_pil_to_latents(image)\n",
    "    else:\n",
    "            image_emb = None\n",
    "\n",
    "    # Adding an unconditional prompt, helps in the generation process\n",
    "    uncond =  text_encode([\"\"] * bs, text.shape[1])\n",
    "    emb = torch.cat([uncond, text], dim=0)\n",
    "\n",
    "    # Setting the seed\n",
    "    if seed: \n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    # Handle image conditioning by modifying initial latents\n",
    "    if image_emb is not None:\n",
    "\n",
    "        scheduler.set_timesteps(steps)\n",
    "\n",
    "        # Convert strength to proper noise strength\n",
    "        # Higher strength = less noise added to image\n",
    "        noise_strength = 1.0 - strength\n",
    "\n",
    "        # Calculate which step to start denoising from\n",
    "        start_step = int(steps * noise_strength)\n",
    "        \n",
    "        # Get the appropriate timestep for noise level\n",
    "        init_timestep = scheduler.timesteps[start_step:start_step+1]\n",
    "        init_timestep = init_timestep.to(\"mps\")\n",
    "        \n",
    "        # Add appropriate amount of noise to the image embedding\n",
    "        noise = torch.randn_like(image_emb)\n",
    "        latents = scheduler.add_noise(image_emb, noise, init_timestep).to(\"mps\").half()\n",
    "        \n",
    "        # Use timesteps starting from the calculated step\n",
    "        timesteps = scheduler.timesteps[start_step:]\n",
    "        \n",
    "        print(f\"Starting img2img from step {start_step}/{steps} (strength: {noise_strength:.2f})\")\n",
    "    else:\n",
    "        # Pure text-to-image: start with random noise\n",
    "        scheduler.set_timesteps(steps) \n",
    "        latents = torch.randn((bs, unet.config.in_channels, dim//8, dim//8))\n",
    "        latents = latents.to(\"mps\").half() * scheduler.init_noise_sigma\n",
    "        timesteps = scheduler.timesteps\n",
    "\n",
    "    print(\"Processing text prompts:\", prompts)\n",
    "    # Initialize array to store norm values\n",
    "    norm_history = []\n",
    "    \n",
    "    # Calculate and store initial norm\n",
    "    initial_norm = torch.norm(latents.view(latents.shape[0], -1), dim=1).mean().item()\n",
    "    norm_history.append(initial_norm)\n",
    "    print(f\"Initial Latents Norm: {initial_norm}\")\n",
    "\n",
    "    clear_memory()\n",
    "\n",
    "    # Iterating through defined steps\n",
    "    for i,ts in enumerate(tqdm(timesteps)):\n",
    "            clear_memory()  # Clear before each step\n",
    "            \n",
    "            # We need to scale the i/p latents to match the variance\n",
    "            inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
    "            \n",
    "            # Predicting noise residual using U-Net\n",
    "            with torch.no_grad(): \n",
    "                    u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n",
    "            \n",
    "            # Performing Guidance\n",
    "            pred = u + g*(t-u)\n",
    "            \n",
    "            # Conditioning the latents\n",
    "            latents = scheduler.step(pred, ts, latents).prev_sample\n",
    "\n",
    "            # Calculate current norm\n",
    "            current_norm = torch.norm(latents.view(latents.shape[0], -1), dim=1).mean().item()\n",
    "            \n",
    "            # Get previous norm from history\n",
    "            previous_norm = norm_history[-1] if norm_history else current_norm\n",
    "            \n",
    "            # Store current norm in history\n",
    "            norm_history.append(current_norm)\n",
    "\n",
    "            print(f\"Step {i+1}/{steps} Latents Norm: {current_norm:.3f}\")\n",
    "\n",
    "            # Check for divergence\n",
    "            if current_norm > previous_norm * 1.5:\n",
    "                print(\"Warning: Latents may be diverging!\")\n",
    "\n",
    "            if save_int and i%10==0: \n",
    "                    image_path = f'steps2/la_{i:04d}.jpeg'\n",
    "                    convert_latents_to_pil(latents)[0].save(image_path)\n",
    "                    display(convert_latents_to_pil(latents)[0])  # Display the new image\n",
    "\n",
    "    # Print norm progression summary\n",
    "    print(f\"\\nNorm progression: {norm_history[0]:.3f} â†’ {norm_history[-1]:.3f}\")\n",
    "\n",
    "    return convert_latents_to_pil(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537558a-12cc-4201-bee4-6b219f199160",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initiating tokenizer and encoder.\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"mps\")\n",
    "\n",
    "## Initiating the VAE\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"mps\")\n",
    "\n",
    "## Initializing a scheduler and Setting number of sampling steps\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "# scheduler.set_timesteps(50)\n",
    "\n",
    "## Initializing the U-Net model\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"mps\")\n",
    "\n",
    "# Call this function before heavy operations\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078d7cd6",
   "metadata": {},
   "source": [
    "<!-- Process:\n",
    "    1. Text Encoding: Converts prompts to CLIP embeddings for conditioning\n",
    "    2. Image Conditioning: Optionally encodes input image to latent space\n",
    "    3. Noise Initialization: Creates random latents with optional image blending\n",
    "    4. Iterative Denoising: Uses U-Net to progressively remove noise over 'steps'\n",
    "    5. Guidance Application: Combines conditional and unconditional predictions\n",
    "    6. Latent-to-Image: Converts final latents back to viewable images\n",
    "\n",
    "Features:\n",
    "    - Classifier-free guidance for better prompt adherence\n",
    "    - Image-to-image generation with controllable influence\n",
    "    - Memory management for Apple Silicon MPS\n",
    "    - Progress monitoring with latent norm tracking\n",
    "    - Intermediate image saving and display\n",
    "    - Divergence detection for stability monitoring\n",
    "\n",
    "Examples:\n",
    "    # Text-to-image generation\n",
    "    images = generate_image([\"A sunset over mountains\"], steps=25, seed=42)\n",
    "    \n",
    "    # Image-to-image with strong conditioning\n",
    "    images = generate_image(\n",
    "        prompts=[\"Turn this into a Van Gogh painting\"],\n",
    "        input_image=\"photo.jpg\",\n",
    "        src_img_influence=0.8,\n",
    "        steps=30\n",
    "    ) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec1277",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images = generate_image(prompts=[\"A fit woman, wearing sunglasses and a Hawaiian shirt, running on a trail in scenic mountains\"], \n",
    "                        g=12, # guidance (0-20)\n",
    "                        seed=42,\n",
    "                        steps=50, \n",
    "                        save_int=True, \n",
    "                        input_image=None,#'img/running_banana_only.jpg', \n",
    "                        strength=1.0,  # Denoising strength for img2img; Low strength = preserve original, High strength = more transformation\n",
    "                        scheduler=scheduler,\n",
    "                        unet=unet\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86da4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in images:\n",
    "    display(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bananas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
